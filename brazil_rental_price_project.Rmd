---
title: "Prediciting House Rental Price"
author: "Eline Morais"
date: "19/06/2020"
output:
  pdf_document: default
  word_document: default
  toc: true
  number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 6, fig.height = 3.5, fig.align = "center")
options(digits = 4,  stringsAsFactors = FALSE)
```

```{r, include=FALSE}
tinytex::tlmgr_install("pdfcrop")
```


## INTRODUCTION

The aim of this data analysis project is to create an automated model of prediction to property rental values without human bias using regression machine learning algorithms.

For the owner of a property, determining the rental price is very important to make his investment worthwhile. If you charge less you will be losing money and if you charge too much, you may not find a tenant willing to pay and the property will be empty for a long period becoming more of a cost, and extending the period of payback.

In the same way, the person who is going to rent, is always looking for the best cost-benefit trying to find a property well located, comfortable and at the lowest possible price.

Knowing that both, "buyers and sellers" have contrary and biased interests, it is necessary to develop an independent technique to calculate the fair price of a house according to several factors that affect the price, such as the physical condition, location, size, etc.

The data source used for this project is obtained from https://www.kaggle.com/rubenssjr/brasilian-houses-to-rent?select=houses_to_rent_v2.csv, also available at Github https://raw.githubusercontent.com/emoraiss/brazilian_rent/master/datasets_554905_1035602_houses_to_rent_v2.csv".

The data files above consists of information from about 10.692 houses to rent in different cities in Brazil and 13 different features. According to Rubens Junior, who published this dataset, the data was collected from a rental website, on 3/20/20.

The dataset was randomly partitioned into two tables by a 90%$\$10% ratio. The first
table with 90% partition was used to build the model while the second table was used as a validation dataset for the would-be developed algorithm.

To measure the assertiveness of the models and define the best approach, the RMSE will be used. RMSE - Residual Mean Squared Error is the square root of the average of squared errors. Error, in turn, is the difference between the prediction and the actual outcome, therefore the lower RMSE the better. 

This project is a part of the final evaluation of students on the Data Science Professional Certificate of Harvardx - Casptone, where each student must choose a dataset on internet and use tools they learned during the course, applying machine learning. 


## EXPLORATORY DATA ANALYSIS - EDA

### Data extraction

From this project we are going to use the following packages. Specific machine learning packages will be loaded at the time of training data later.

```{r libraries, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret) 
library(readr)
library(httr)
library(dplyr)
library(ggplot2) 
library(ggthemes) 
library(gridExtra) 
library(stringr)
library(knitr)
library(lubridate)
library(rvest)
library(matrixStats)
library(purrr)
```

Files will be downloaded from url above, and translating them into a structured tidy format is illustrated as shown below:

```{r data, warning=FALSE, message=FALSE}
#Accessing the data - Github
tmp<- tempfile()
download.file("https://raw.githubusercontent.com/emoraiss/brazilian_rent/master/datasets_554905_1035602_houses_to_rent_v2.csv",tmp)
data<-read_csv(tmp)
file.remove(tmp)

```

### Data structure

We can start confirming dimensions of dataset: 

```{r dim, echo= FALSE}
dim(data)

```

And having a look on features:

```{r features, echo=FALSE}
names(data)

```

The first step in building an algorithm is to better understand the outcomes and features. Below follows a brief description of them:

  #Outcome:
  
  -rent amount (R$) - Rent amount
  
  #Features:
  
  -city - City where the property is located 
  
  -area - Property area in square meters
  
  -rooms - Quantity of rooms
  
  -bathroom - Quantity of bathrooms 
  
  -parking spaces - Quantity of parking spaces
  
  -floor - In which floor the apartment is located, in case of apartment
  
  -animal -  Accept or not accept animals?
  
  -furniture -  Is furbished or not? 
  
  -hoa (R$) - Homeowners association tax - condominium that makes and enforces rules for the properties and their residents, monthly paid for common-area and facilities upkeep. 
  
  -property tax (R$) - By definition, property tax is the tax paid on property owned by an individual or other legal entity, such as a corporation. The tax is usually calculated by a local government where the property is located and paid by the owner of the property, based on the value of the owned property, including land.
  
  -fire insurance (R$) - is a property coverage that pays for damages to property and other losses you may suffer from a fire.
  
  -total (R$) - Total amount of 4 previous values

As a good practice and to facilitate the coding process we will remove spaces, punctuation and curly brackets from titles, resulting on these news titles:

```{r change_names, echo=FALSE}
#Changing titles
colnames(data) <- gsub(pattern = "[[:punct:]]|R", replacement = "", colnames(data))
colnames(data) <- colnames(data) %>% str_trim() %>% str_replace_all(.,"\\s", "_")
colnames(data)

```

Looking at the summary we can observe there is a small variability in the number of rooms, bathrooms and parking spaces and, on the other hand there is a wide variation in areas, values of rent amount, and other taxes and fees.

```{r summary_data, echo=FALSE}
summary(data)
```

We also see that "floor" is classified as "character" and as we can see below for most properties this field is filled with "-". Once we do not have one field to identify the type of property (house or apartment), we could suppose, observations where floor= "-", are houses, what means "do not apply". So, let us change this "-" to "0", and change this field to numeric.

```{r count_floor, echo=FALSE, message=FALSE}
data %>% group_by(floor) %>% summarise(n=n()) %>% arrange(desc(n)) %>% head() %>% knitr::kable()
```

```{r as-numeric_floor feature, echo=FALSE}
data <- data %>% mutate(floor=as.numeric(ifelse(floor=="-", 0, floor)))

```

We see that there is none "NA".

```{r any_NA}
sum(is.na(data))
```

Our dataset contains houses in 5 different cities in Brazil, with mean area about 150m2, mean price about R$3.900,00, 11 different number of rooms and bathrooms, and 35 different options of floor.

```{r summary_2, echo=FALSE}
data %>% summarise(Cities= n_distinct(city), Mean_area= mean(area), Room_options= n_distinct(rooms), Parking_options= n_distinct(parking_spaces),
                   Floor_options= n_distinct(floor), Mean_rental= mean(rent_amount)) %>% knitr::kable()

```

A more detailed look at the data reveals that most of properties are concentrated in São Paulo, which is responsible for 55% of available houses. The number of properties by cities can be seen on the table below.

```{r cities, echo=FALSE}
data %>% group_by(city) %>% summarise(n= n(), "%" = (n*100/10692)) %>% arrange(desc(n)) %>% knitr::kable()

```

### Training/Test data set

Before starting the exploratory analysis of the data it is necessary to split the data set into training and validation set.

To create our train and test set we will use "createDataPartition" from caret package, setting seed = 1 to be reproducible. We will take a random sample of 10% as a validation data while the remaining
90% is allocated to build the model.

```{r Create build and validation,include=TRUE, message=FALSE, warning=FALSE}
# Validation set will be 10% of data
# if using R 3.5 or earlier, use `set.seed(1)` instead
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = data$rent_amount, times = 1, p = 0.1, list = FALSE)
build<- data[-test_index,]
validation <- data[test_index,]
```

It is important to note that the validation dataset will be considered as an unknown basis to evaluate the accuracy of our final model, trained and tested only with the data from the build dataset. Therefore, the validation set will not be considered in the exploratory analysis.

The high ratio 90 $\$ 10 was defined due to the relatively small size of our dataset. So, now our "build" dataset contains 9.621 houses and 13 features. 

```{r dim_build, echo= FALSE}
dim(build)
```

### Data Visualization

We are going to examine the outcome and its relations with each feature starting by area. 

* Area

```{r area_by_city, echo= FALSE}
area <- build %>% group_by(city) %>%  qplot(city, area, data = ., geom = "boxplot") +  scale_y_log10()+theme_bw() +ggtitle("Area by City")+ theme(axis.text.x = element_text(angle = 90, hjust = 1))

rent<- build %>% group_by(city) %>%  qplot(city, rent_amount, data = ., geom = "boxplot") + scale_y_log10() +theme_bw() +ggtitle("Rent by City") +theme(axis.text.x = element_text(angle = 90, hjust = 1))

grid.arrange(area, rent, ncol = 2)
```

Comparing area by city, we can see that São Paulo and Belo Horizonte have larger ranges, but in general 50% of properties have areas between 50m2 and 200m2.

Since by definition a mansion is around 2.000 m2, we will consider properties that have an area equal or larger than that as errors or outliers and will remove them. Our dataset has 4 of them.

```{r highest_areas, echo=FALSE}
build %>% select(city, area, rent_amount)  %>% arrange(desc(area)) %>% head() %>% knitr::kable()
```

New dimension:

```{r removing_outliers_area}
remove_index<- which(build$area>=2000)

build<- build[-remove_index,]

dim(build)
```

At the same time we observe that there are 60 properties smaller than 20m2 or 695 smaller than 30m2. But we are going to keep them once its quantity is relevant and because it is normal to find studios or even single rooms to rent at this size.  

Comparing rent amount by city, we can see that São Paulo presents the most expensive properties, followed by Belo Horizonte and Rio de Janeiro, which have similar behavior, and after Porto Alegre and Campinas presenting the cheapest prices.


```{r summary_rental, include= FALSE}
build %>% group_by(city) %>% summarise(median= median(rent_amount), first= summary(rent_amount)[2], third=summary(rent_amount)[4])
```

Let us have a look on the highest rental prices.

```{r highest_prices, echo=FALSE}
build %>% select(city, area, rooms , bathroom, parking_spaces, property_tax,rent_amount)  %>% filter(rent_amount>10000) %>% arrange(desc(rent_amount)) %>% head(., 10) %>% knitr::kable()
```

We see a small house in disagreement with the rest of the "select" group. Let us take rent values higher than 20k as outliers.

```{r outliers_rent_amount, echo= FALSE, include=FALSE}
remove_index<- which(build$rent_amount>20000)

build<- build[-remove_index,]

dim(build)
```

Now let us see their distributions: 

```{r rental_area_dist, echo= FALSE, message=FALSE}
rent<- build %>% ggplot(aes(rent_amount)) + geom_histogram(binwidth= 0.1, color= "black", fill= "lightblue") + theme_bw()+
  scale_x_log10() + ggtitle("Rent distribution")


area<- build %>% ggplot(aes(area)) + geom_histogram(binwidth = 0.1, color= "black", fill= "lightblue") +
  scale_x_log10() +theme_bw()+ ggtitle("Area distribution")
grid.arrange(area,rent, ncol = 2)
```

```{r summary_area, include=FALSE}
build %>% group_by(city) %>% summarise(median= median(area), first= summary(area)[2], third=summary(area)[4])

```

Assuming that the properties have the same pattern, one could expected that the larger the property, higher the rent would be, once a common unit of measure is value per square meter. On the graphic below we can see if there is a positive correlation between them. 

```{r Rent_Area_plot, echo= FALSE, message=FALSE}
build %>% ggplot(aes(rent_amount, area)) +scale_x_log10()+scale_y_log10()+ geom_point() +theme_bw()+ geom_smooth(method= "lm") + ggtitle("Rent vs Area")

```

We can see below that all cities show a strong correlation, larger than 0.6. 

```{r Rent_Area_bycity, echo= FALSE, message=FALSE}
build %>% ggplot(aes(rent_amount, area)) +scale_x_log10()+scale_y_log10()+ geom_point()+theme_bw()+ geom_smooth(method= "lm") + facet_wrap(~city) + ggtitle("Rent vs Area by City")

```
```{r rent_area_table, echo=FALSE, message=FALSE, include=FALSE}
build %>% group_by(city) %>% summarise(cor= cor(area, rent_amount)) %>% arrange(desc(cor)) %>% knitr:: kable()
```

* Rooms and Bathrooms

We have seen at the first summary that the minimum and maximum number of rooms (between 1 and 13) and bathrooms (between 1 and 10) are quite similar. Let us check how they are distributed and correlated. 

```{r room_bath_dist, echo= FALSE}
room<- build %>% group_by(rooms) %>% ggplot(aes(rooms)) + geom_histogram(binwidth= 0.5,color= "black", fill= "lightblue") +scale_y_sqrt() + theme_bw() + ggtitle("Rooms distribution")

bath<- build %>% group_by(bathroom) %>% ggplot(aes(bathroom)) + geom_histogram(binwidth= 0.5, color= "black", fill= "lightblue") +scale_y_sqrt()+ theme_bw() + ggtitle("Bathrooms distribution")
grid.arrange(room, bath, ncol=2)
```

Only about 2% of houses has more than 5 rooms or bathrooms. In a boxplot we can see the range of bathrooms for each quantity of room.

```{r room_bath_plot, echo= FALSE}
build  %>% ggplot(aes(rooms, bathroom, group= rooms)) +
    geom_boxplot() +theme_bw()+ggtitle("Rooms vs Bathroom") 
```

We can think that a low standard apartment/house may have fewer bathrooms than rooms, in spite of high standard properties, in addition to having one bathroom per room, there may be a toilet, a maid's bathroom, a leisure area bathroom, among others.

Looking closer at some points in the graphic we can find other outliers. Small houses (area) with a big quantity of bathroom or rooms, making no sense. Let us remove them.

```{r outliers_rooms, echo=FALSE}
build %>% filter(rooms==5 & bathroom==1|rooms==2 & bathroom>6) %>% select(city, area, rooms, bathroom, parking_spaces, floor, rent_amount) %>% knitr::kable()
```

```{r removing_outliers_room, echo=FALSE, include=FALSE}
remove_index<- which(build$rooms==5 & build$bathroom==1|build$rooms==2 & build$bathroom>6) 
build<- build[-remove_index,]
dim(build)
  
```

Now let us see the relation of room and bathroom with rental price.

```{r room_baths_vs_rental, echo=FALSE}
room<- build %>% group_by(rooms) %>%  ggplot(aes(rooms, rent_amount, group= rooms)) + geom_boxplot() +  scale_y_log10()+theme_bw() +ggtitle("Rooms vs Rental price")

bathroom<- build %>% group_by(bathroom) %>% ggplot(aes(bathroom, rent_amount, group= bathroom)) + geom_boxplot()+ scale_y_log10()+theme_bw() +ggtitle("Bathrooms vs Rental price")

grid.arrange(room, bathroom, ncol = 2)
```

By these plots we confirm that rental price is highly related to the number of rooms and mainly to the number of bathrooms.

* Parking Spaces

We believe they are quite related to area and rental price as well, since highest pattern houses may have more parking spaces. 

```{r Parking_Dist, echo= FALSE}
build %>% group_by(parking_spaces) %>% ggplot(aes(parking_spaces)) + geom_histogram(binwidth = 0.5, color="black", fill="lightblue") +scale_y_sqrt()+ theme_bw() + ggtitle("Parking Spaces distribution")


```

By the plot we can see most properties has less than 4 parking spaces. Now let us have a look at boxplot to see the range of area and rooms to each quantity of parking space. 

```{r parking_box, echo=FALSE}
area<- build %>% group_by(parking_spaces) %>% ggplot(aes(parking_spaces, area, group= parking_spaces)) + geom_boxplot()+ scale_y_log10()+theme_bw() +ggtitle("Parking Spaces vs area")
rooms<-build %>% group_by(parking_spaces) %>% ggplot(aes(parking_spaces, rooms, group= parking_spaces)) + geom_boxplot()+ scale_y_log10()+theme_bw() +ggtitle("Parking Spaces vs rooms")
grid.arrange(area, rooms, nrow=2)
```

Looking closer to some outliers we can confirm they should be removed. There are 8 houses with only one room and 4 or more parking spaces. Also, there are 3 other houses with less than 100m2, 2 rooms and more than 4 parking spaces. Let us remove these outliers as well.

```{r outliers_parking, echo= FALSE}
build %>% filter(parking_spaces>=4 & area<100 & rooms<3|rooms==1 & parking_spaces>3) %>% select(city, area, rooms, bathroom,hoa,  parking_spaces, rent_amount) %>% arrange(desc(parking_spaces)) %>% knitr:: kable()

```

```{r removing_outliers_parking, echo=FALSE, include=FALSE}
remove_index<- which(build$parking_spaces>=4 & build$area<100 & build$rooms<3|build$rooms==1 & build$parking_spaces>3)
build<- build[-remove_index,]
dim(build)

```

On the graphic below we can see how parking spaces are related to rental price.

```{r Parking_vs_Rent, echo=FALSE}
build %>% group_by(parking_spaces) %>% ggplot(aes(parking_spaces, rent_amount, group=parking_spaces)) + geom_boxplot() + scale_y_log10()+ theme_bw() + ggtitle("Parking spaces vs Rental price")

```

We see an increasing price variation with the increase in the number of parking until reaching 4. The same behave is found even when detailed by city.

* Furniture

We can think that prices of furnished houses are more expensive due to the investment made. But let us see what data shows. Furnished properties represent about 25% of our dataset.  

```{r furniture, echo=FALSE}
hist<- build %>% group_by(furniture) %>% ggplot(aes(furniture)) + theme_bw()+geom_bar(color= "black", fill= "lightblue") 
box<- build %>% group_by(furniture) %>% ggplot(aes(furniture, rent_amount)) + geom_boxplot() + scale_y_log10() + theme_bw() + ggtitle("Furniture vs Rental price")
grid.arrange(hist, box, ncol= 2)
```

Despite knowing that the rental price varies based on many other variables, we can confirm that the furnished properties have, on average, a higher price, although the range for furnished and unfurnished is practically the same.

* Animal

Although it is a very relevant factor in the decision to choose a home for those who have a pet, we do not expect the price to be impacted. Let us see the plots. 

```{r animal, echo= FALSE}
pet1<- build %>% group_by(animal) %>% ggplot(aes(animal)) + theme_bw()+geom_bar(color= "black", fill= "lightblue") + ggtitle("Animal Policy")

pet<- build %>% group_by(city) %>% ggplot(aes(animal, rent_amount)) + geom_boxplot() + scale_y_log10() + theme_bw() +ggtitle("Animal Policy vs Rental price")
grid.arrange(pet1, pet, ncol=2)
```

Most properties accept animals and by the chart we can confirm that there is not such variation in relation to the rental price.

* Floor

First let us see the highest floor available.

```{r outlier_floor, echo=FALSE}
build %>% arrange(desc(floor)) %>% select(city, area,floor) %>% head() %>% knitr::kable()

```

It is unlikely that this information is true once the highest buildind in brazil has 46 floors. Some taller buildings are expected to be completed in 2020. So for now we will change floors higher than 46 (301 and 51) to 31.

```{r change_floors,echo= FALSE, include= FALSE}
build<- build %>% mutate(floor= ifelse(floor==301|floor==51,31,floor))
build %>% arrange(desc(floor))%>% select(city, area,floor) %>% head()
```

Let us see the distribution. Once we have changed floor with"-" to "zero", assuming they are houses or that there is a lack of information, let us filter the floor > 0 and see the distribution and relation to price.

```{r floor_dist, echo= FALSE}
floor<- build %>% filter(floor>0)  %>% arrange(floor) %>% ggplot(aes(floor))+ geom_histogram(binwidth= 1, color= "black", fill= "lightblue") + theme_bw()+ ggtitle("Floor distribution")
floor2<- build %>% filter(floor>0 &floor<30)  %>% arrange(floor) %>% ggplot(aes(floor, rent_amount, group= floor))+ geom_boxplot() + scale_y_log10() + theme_bw()+ ggtitle("Floor vs Rental")
grid.arrange(floor, floor2, nrow=2)
```

As samples of floor taller than 30 is not representative, we will take them off from the second part of plot. We can observe rental prices are quite similar in every range of floor.

* Property tax

Let us have a look at the highest values:

```{r high_property, echo= FALSE}
build %>% select(city, area, property_tax,  `hoa`,rent_amount) %>% arrange(desc(property_tax)) %>% head(., 8) %>% knitr::kable()

```

We can see that the first five values seem off putting, since that or the area is small in comparison to the thers, or the property tax is much higher than other properties with similar area and rent value of the same city. So, we will take these 5 properties off. 

```{r, ouliers_proptax, echo= FALSE, include= FALSE}
remove_index<- which(build$property_tax>9500)
build<- build[-remove_index,]
dim(build)

```

And now let us analyze the lowest values. There are 1.448 properties where the tax is equal to zero.

```{r count_proptax_0, echo=FALSE, include=FALSE}
build %>% filter(property_tax==0) %>% nrow()
```

```{r area_rent_prop_0, echo= FALSE}
build %>% select(city,area, property_tax,  `hoa`,rent_amount) %>% filter(property_tax==0) %>% arrange(desc(area)) %>% ggplot(aes(area, rent_amount)) + geom_point() + theme_bw()+ ggtitle("Area vs Rental (Property tax = 0)")

```

We can see that most of those houses which do not have property taxes are located in the lower left corner of the graph, which means they are buildings with a smaller area and lower value of rent.

Let us understand a little more about this tax.

In Brazil this tax is called IPTU and each city has a specific law that defines the forms of calculations and exemption cases. In general, we see that the amount due is related to the venal value of property which is determined by the venal value of the land, for territorial properties and, by adding the venal values of the land and construction, for building properties. The venal value of the land is obtained by multiplying the lot's area by the value of the land's m²; correction factors may be applied to this value. 

Doing a quick search on the laws of these 5 cities, we found:

* Rio de janeiro - With the validity of Law 6,250 / 2017, residential units with a venal value of up to  R $ 58,802.00 are exempt from IPTU.

* São Paulo  - Exemptions and discounts for the property's market value are automatically applied the value of the property is up to R $ 160 thousand.
             - Retirees and pensioners, cultural entities, sports associations and Societies Friends of Neighborhoods, among others, may apply for exemption from IPTU if they prove the requirements determined by law. In this case the property's value of up to R $ 1,310,575.00.
             - The tendency is for properties with a smaller built area to be exempt, but the exemption is not related to the construction footage, but to the property's market value (which depends on several factors, in addition to the built area).

* Porto Alegre - The exemption is integral for IPTU for properties with a venal value of up to 100,000 (one hundred thousand).
               - Retired, inactive, pensioners and people with disabilities are exempt from paying the Porto Alegre Property and Land Tax (IPTU).

* Belo Horizonte - Properties with a value of up to R $ 66,601.98 are exempt from tax.

* Campinas - Properties for popular housing registered in the Horizontal Residential (RH) category with a total built area not exceeding 80.00m² (eighty square meters) or in the Vertical Residential (RV) category with a total built area not exceeding 50.00m² (fifty square meters), without territorial area surplus, the venal value of which does not exceed 60,000.
           - Retirees, pensioners and beneficiaries of Social Protection for the Elderly, Social Protection for Persons with Disabilities and Lifetime Monthly Income, in relation to the property belonging to their assets classified in the strictly residential category and where they actually reside, conditioning the person legally benefited to attend some rules.

So in summary, we can see that there is a convergence in exemption from fees for lower value properties, sometimes considering the area as well, however there are some other cases to exempt, like for elderly people that meet certain income criteria and for philanthropic and government support entities. Which means our data shown in the last graph does make sense.

Another way to confirm that is looking to graph below, where we can see most of properties that have property tax = 0 has area less than 150m2 in all cities. 

```{r area_city_prop0, echo=FALSE}
# property tax== 0
build %>% filter(property_tax==0) %>% select(city, area, property_tax) %>% ggplot(aes(city, area)) + geom_boxplot() + theme_bw() + ggtitle("Area by city (Property tax = 0)")

```

Now let us see what happens to properties where the property tax is due. 

```{r area_proptax, echo=FALSE}
build %>% group_by(city) %>% filter(property_tax != 0) %>% ggplot(aes(area, property_tax)) + geom_point() + scale_x_log10()+scale_y_log10() +theme_bw()+ facet_wrap(~city)+theme_bw() + ggtitle("Area vs property tax")

```

The basic and most used rule to calculate rent is to apply a value between 0.5% and 1%, per month, on the market value of the property. This means that if your property is worth 100 thousand the rent should be between R 500 and R $ 1,000 per month.

Therefore, as there is no feature with the market value of the property, we can infer that the market value is related to the venal value, which in turn impacts the value of the tax. That is, the higher the amount of tax to be paid, the more valuable the property is and consequently its rental price. 

Let us see if data confirms it looking at how they behave graphically.

```{r rental_vs_proptax, echo=FALSE}
build %>% group_by(city) %>% filter(property_tax != 0) %>% ggplot(aes(rent_amount,property_tax)) + geom_point() + scale_x_log10()+scale_y_log10() +theme_bw()+ facet_wrap(~city)+theme_bw() + ggtitle("Rental price vs property tax")
```

* Hoa - homeowner’s association

As the previous features let us see the lowest and highest values. 

There are 2.138 properties with hoa= 0, what can be used to define if a property is an independent house or whether it is an apartment or a condominium house, once we do not have the field about the kind of home is about.

```{r hoa_0, echo=FALSE, include=FALSE}
build %>% filter(`hoa`==0) %>% nrow()
```

Looking at the highest values we see that it is necessary to take the first 3 values off, once they do not make any sense, they are too high to be paid monthly if compared with area, property tax or rental price.

```{r highest_hoa, echo=FALSE}
build %>% filter(hoa>0) %>% select(city, area, property_tax,  hoa,rent_amount) %>% arrange(desc(hoa)) %>% head(., 8) %>% knitr::kable()
```

```{r remove _outliers_hoa, echo=FALSE, include=FALSE}
remove_index<- which(build$hoa>30000)
build<- build[-remove_index,]
dim(build)
  
```

We will create a new feature "condominium" to classify properties in two classes, those who do pay the fee and those who do not.

```{r condominium, echo=FALSE}
build %>% mutate(condominium= ifelse(hoa==0, "no","yes")) %>% ggplot(aes(condominium, rent_amount)) + geom_boxplot() +theme_bw()+ scale_y_log10()+ facet_wrap(~city)+ ggtitle("Condominium vs Rental price)") 

```

Properties that do not have a condominium fee have higher median rental values in Belo Horizonte, Campinas and Porto Alegre. This is justified because for the tenant what really matters is the total monthly amount to be paid, which includes the condominium fee.

For those who have hoa fee, we can see that São Paulo and Rio de Janeiro have highest values of hoa and highest standard deviation.

```{r hoa_bycity, echo=FALSE}
build %>% filter(hoa<32000 & hoa>1) %>% ggplot(aes(city, hoa)) + geom_boxplot() +scale_y_log10()+theme_bw()+ggtitle("Hoa by city") 

```

```{r hoa_summary,  include=FALSE}
build %>% filter(hoa<32000 & hoa>1) %>% group_by(city) %>% summarise(avg= mean(hoa), sd= sd(hoa), median= median(hoa)) %>% knitr::kable()
```

Now let us see the relation between hoa and rental price.

```{r hoa_vs_rent, echo= FALSE}
build %>% filter(hoa<32000 & hoa>1) %>% ggplot(aes(hoa, rent_amount)) + geom_point() +theme_bw()+ scale_y_log10() + scale_x_log10()+ggtitle("Hoa fee vs Rental price") 

```

* Fire Insurance

In relation to fire insurance we can observe there is no such a variation like the previous taxes. For most of properties (98,7%) this fee is less or equal to R$200 which means its value is not too related to area or value of property. 

```{r fire_dist, echo= FALSE, message=FALSE}
build %>% ggplot(aes(fire_insurance)) + geom_histogram(color="black", fill="lightblue") +scale_y_sqrt()+ theme_bw() + ggtitle("Fire Insurance Distribution")

```
```{r fire<200, include=FALSE}
build %>% filter(fire_insurance<=200) %>% nrow()/nrow(build)

```

We can see also, the ones which have the highest values are the ones that have the highest rent amounts in a perfect correlation. Which implies that this feature is calculated based on the rent amount, therefore is not an independent feature.

```{r fire_vs_Rent, echo= FALSE}
build %>% ggplot(aes(fire_insurance, rent_amount)) + geom_point()+ scale_y_log10() +scale_x_log10()+ theme_bw() + ggtitle("Fire insurance vs Rental price")

```

* Total

Considering that this feature is also a dependable variable, equal to the sum of rental price and all previous presented tax and fees, we are going to remove it, leaving the following features remaining:

```{r remove_total_fire, include=TRUE}
build<- select(build,- c(total, fire_insurance))
names(build)
```

Now that we have seen all features we can start to build our model. 

## Modelling

We are going to use different regression algorithms to see which one performs better. Models will be evaluated and compared by using the function "postResample" that estimates the root mean squared error (RMSE), simple R2, and the mean absolute error (MAE). However we will choose the model of smaller RMSE.

Some definitions:
- RMSE - Residual Mean Squared Error, the square root of the average of squared errors.
- Rsquared - The R² indicates how much that model explain the observations well.
- MAE - The mean absolute error is calculated using mean(abs(pred-obs)). We can use the model to predict a "fair" rent price for a property and use the MAE as a tolerance. This would help a buyer to find good opportunities and run away from bad ones.


### Pre processing

First we will see if there is some predictor zero-variance. Predictors when the data are split into cross-validation/bootstrap sub-samples or that a few samples may have an undue influence on the model. These “near-zero-variance” predictors may need to be identified and eliminated prior to modeling.

```{r nzv}
nzv<- nearZeroVar(build)
nzv
```

#### Correlation

Most models may benefit from reducing the level of correlation between the predictors. Let us take a look into the correlation  between all numeric features.

```{r corr_plot, echo= FALSE, message=FALSE, warning=FALSE}
library(ggcorrplot)
vars_cont <- build %>% select_if(is.numeric)
corr <-  cor(vars_cont, use = 'complete.obs')
options(repr.plot.width=20, repr.plot.height=20)
ggcorrplot(corr, lab = TRUE, colors = c("aquamarine", "white", "dodgerblue"), 
           show.legend = F, outline.color = "gray", type = "upper", 
           tl.cex = 12, lab_size = 3, sig.level = .1) + ggtitle("Correlation between variables")+
  labs(fill = "Correlation")
```

We see the highest correlation, higher than 0.75 correlation between area and bathrooms. We can see below the correlation summary.

```{r summary_cor}
vars_cont <- build %>% select_if(is.numeric)
buildCor <- cor(vars_cont)
summary(buildCor[upper.tri(buildCor)])
```

The code below shows the highest correlation after removing predictors with absolute correlations above 0.75 and the remaining predictor names.

```{r high_cor}
highlyCor <- findCorrelation(buildCor, cutoff = .75)
vars_cont <- vars_cont[,-highlyCor]
names(vars_cont)
buildCor2 <- cor(vars_cont)
summary(buildCor2[upper.tri(buildCor2)])
```

So we are going to take the predictor "area" off.

```{r removing_area, echo=FALSE}
build<- select(build,-area)

```

We do not find any linear dependency among features

```{r linear_dependency}
comboInfo <- findLinearCombos(vars_cont)
comboInfo

```

### Creating train and test set

After removing outliers and the high correlated features, 9.593 houses remaining at our build dataset with 9 features and 1 outcome. 

```{r dimension_final, echo=FALSE}
dim(build)
```

Now we are going to split the build dataset in two parts in a ratio 90/10 called "train" and "test" datasets.

```{r train_test_build, message=FALSE, warning=FALSE}
#Create train and test set
set.seed(27, sample.kind = "Rounding")
test_index <- createDataPartition(y =build$rent_amount, times = 1, p = 0.1, list = FALSE)
train<- build[-test_index,]
test <- build[test_index,]
```

Before training models with machine learning algorithms we will set seed = 35 so that they all use the same sample and results can be compared.

To reduce the risk of overtraining and randomness we are going to use Cross validation technique, which have as general idea to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error. 

```{r control_cross_validation, include=TRUE}
control <- trainControl(method = "cv",
                              number = 10, p=0.9)
```

### Machine Learning Algorithms

#### 1- Linear Regression

Sometimes it can be too rigid to be useful and but for some challenges it works rather well. We will define its results as our baseline to compare to other models.

```{r train_regression, message= FALSE, warning=FALSE}
ini_time<- Sys.time()
set.seed(35, sample.kind = "Rounding")
train_lm<- train(rent_amount~., method= "lm", data = train, preProcess= "scale",trControl = control)
end_time<- Sys.time()
train_lm
print(end_time-ini_time)
```

Using the model to predict on the test set we obtain a RMSE = 2079.

```{r results_lm}
y_hat_lm<- predict(train_lm, test)
lm_results<- postResample(y_hat_lm, test$rent_amount)
lm_results
```

Predictions could be easily calculated by a linear function with the following coefficients:

```{r lm_coef, echo=FALSE}
train_lm$finalModel$coefficients %>% knitr::kable()
```

And now we can see what were our biggest mistakes, comparing prediction to actual data.

```{r plot_lm, echo= FALSE, message=FALSE}
test %>% mutate(y_hat= y_hat_lm) %>%  ggplot(aes(y_hat, rent_amount)) + geom_point() + geom_smooth(method= "lm")  +theme_bw()+ggtitle("Linear Regression Results")

```

We can see that a few outliers are responsible for much of our error, impacting the RMSE. Let's have a look in the biggest mistakes.

```{r biggest_mistakes_lm, echo=FALSE}
test %>% mutate(y_hat= y_hat_lm, error= abs(rent_amount-y_hat)) %>% select(city,bathroom,rooms, property_tax, rent_amount, y_hat, error) %>% arrange(desc(error)) %>% head() %>% knitr::kable()
```

Let us try to improve using others models.

#### 2- K-Nearest Neighbors 

*method = "knn"*

We can control the flexibility of our Knn estimate through the k parameter: larger ks result in smoother estimates, while smaller ks result in more flexible and more wiggly estimates.

```{r train_knn, echo= FALSE, warning=FALSE, include=FALSE}
#picking k 
set.seed(35, sample.kind = "Rounding")
ini_time<- Sys.time()
train_knn<- train(rent_amount~., method= "knn", data= train, preProcess= "scale",tuneGrid= data.frame(k=seq(10,75,5)), trControl= control)
end_time<- Sys.time()
train_knn
print(end_time-ini_time)

```

We pick k = 25 as shown on the plot below.

```{r Knn_best_tune, echo=FALSE}
ggplot(train_knn, highlight= TRUE) +theme_bw()+ ggtitle("Knn - Best Tune")
```

Now we can predict on the test dataset to see the results.

```{r results_knn, echo=FALSE}
y_hat_knn<- predict(train_knn,test, type = "raw")
knn_results<- postResample(y_hat_knn, test$rent_amount)
knn_results
```

There was an improvement about 5% if compared to linear regression. Let's see if we can do better.

#### 3- Regression tree 

*method = "rpart"*

On this model we have the tuning parameter: cp (Complexity Parameter). 

```{r train_rpart, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(rpart)
set.seed(35, sample.kind = "Rounding")
ini_time<- Sys.time()
train_rpart<- train(rent_amount~., method= "rpart", data= train,trControl= control, preProcess= "scale" ,tuneGrid = data.frame(cp= seq(0,0.06, len=25)), minsplit= 5)
end_time<- Sys.time()
train_rpart
print(end_time-ini_time)
```

```{r besttune_rpart, echo=FALSE}
ggplot(train_rpart, margin= 0.1, highlight=TRUE) + theme_bw() + ggtitle("Cp - Best Tune")
```

Cp was defined = 0.0025 but RMSE resulted in 2239, higher than previous models. Let us have a look on the final model of Regression Tree.

```{r final_model_rpart, echo=FALSE, message=FALSE, warning=FALSE}
library(rpart.plot)
rpart.plot(train_rpart$finalModel,box.palette="RdBu", shadow.col="gray", nn=TRUE, fallen.leaves=FALSE, tweak=1.3)

```

Although results on training was worse let us save the RMSE on the test set.

```{r results_rpart, echo=FALSE}
y_hat_rpart<- predict(train_rpart, test)
rpart_results<- postResample(y_hat_rpart, test$rent_amount)
rpart_results
```

#### 4- Random Forest 

*method = "rf"*

We have tested some values for ntree and mtry. The best results were obtained with ntree=100, mtry=3 and without pre-processing features. 

```{r train_rf, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(randomForest)
#Tuning parameters:
#mtry (#Randomly Selected Predictors)
#pre-processing RMSE=2000.782 , no pre-processing RMSE= 2000.133.
ini_time<- Sys.time()
set.seed(35, sample.kind = "Rounding")
train_rf<- train_rf<- train(rent_amount~., method= "rf", ntree=100, data= train, tuneGrid= data.frame(mtry=c(2,3,4,5,6)), trControl= control, nSample=5000)
end_time<- Sys.time()
train_rf
print(end_time-ini_time)

```

```{r mtry_besttune, echo=FALSE}
ggplot(train_rf, highlight= TRUE) + theme_bw() + ggtitle("Mtry - Best Tune")
```

One disadvantage of random forests is that we lose interpretability. An approach that helps with interpretability is to examine variable importance. To define variable importance we count how often a predictor is used in the individual trees. 

```{r varImp_rf, echo=FALSE}
plot(varImp(train_rf))
```

So far Random Forest presents the best results. Let us calculate RMSE on the test set.

```{r results_rf, echo=FALSE}
y_hat_rf<- predict(train_rf, test)
rf_results<- postResample(y_hat_rf, test$rent_amount)
rf_results
```

We had an improvement of more 4% in relation to previous best model Knn, or 8% in relation to Linear Regression. Let us plot the results.

```{r rf_plot_results, echo=FALSE}
test %>% mutate(y_hat= y_hat_rf) %>%  ggplot() + geom_point(aes(y_hat, rent_amount)) + theme_bw()+ggtitle("Predictions Random Forest")
```

We still have a few outliers responsible for most of variance, but they are smaller than the previous models as shown below.

```{r errors_rf, echo=FALSE}
test %>% mutate(y_hat= y_hat_knn, error= abs(rent_amount-y_hat)) %>% select(city, property_tax,bathroom, hoa, parking_spaces, rent_amount, y_hat, error) %>% arrange(desc(error)) %>% head() %>% knitr::kable()

```

#### 5- Random Forest 

*method = "Rborist"*

```{r train_Rborist, echo= FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(Rborist)
#Tuning parameters:
#predFixed (#Randomly Selected Predictors)
#minNode (Minimal Node Size)
ini_time<- Sys.time()
set.seed(35, sample.kind = "Rounding")
train_rb <-  train(rent_amount~.,
                   method = "Rborist",
                   nTree = 100,
                   preProcess= "scale",
                   trControl = control,
                   tuneGrid = data.frame(minNode= seq(5,250,25), predFixed= 5), 
                 data= train)
end_time<- Sys.time()
train_rb
print(end_time-ini_time)

```


```{r rb_besttune, echo=FALSE}
ggplot(train_rb, highlight= TRUE) + theme_bw() + ggtitle("MinNode - Best tune")
```

Although the performance on the training data was very similar but inferior to the previous model, on the test dataset, Rborist got better results.

```{r rb_results, echo= FALSE}
y_hat_rb<- predict(train_rb, test)
rb_results<- postResample(y_hat_rb, test$rent_amount)
rb_results
```

We can see one change into the features importance.

```{r varImp_rb, echo=FALSE}
plot(varImp(train_rb))
```

#### 6- Support Vector Machines with Radial Basis Function Kernel 

*method = "svmRadial"*

```{r train_svm, echo=FALSE, warning=FALSE, message=FALSE, include=FALSE}
#method= "svmRadial"
#Tuning parameters:
#sigma (Sigma)
#C (Cost)
library(kernlab)
set.seed(35, sample.kind = "Rounding")
ini_time<- Sys.time()
train_svm<- train(rent_amount~., method= "svmRadial", data= train, preProcess= "scale", trControl= control)
end_time<- Sys.time()
train_svm
print(end_time-ini_time)

```

```{r cost_besttune, echo=FALSE}
ggplot(train_svm, highlight= TRUE) + theme_bw() + ggtitle("Cost - Best Tune")
```

Resulting in a RMSE on the test set equal to 1962 on the test set.

```{r svm_results, echo=FALSE}
y_hat_svm<- predict(train_svm, test)
svm_results<-postResample(y_hat_svm, test$rent_amount)
svm_results
```

#### 7- Model Tree

*method = "cubist"*

```{r train_model_tree, echo= FALSE, warning=FALSE, message=FALSE, include=FALSE}
#model tree cubist
#Tuning parameters:
#committees (#Committees)
#neighbors (#Instances)
library(Cubist)
set.seed(35, sample.kind = "Rounding")
ini_time<- Sys.time()
train_cub<- train(rent_amount~., method= "cubist", data= train,preProcess= "scale", trControl= control)
end_time<- Sys.time()
train_cub
print(end_time-ini_time)
```


```{r besttune_model_tree, echo=FALSE}
ggplot(train_cub, highlight= TRUE) + theme_bw() + ggtitle("Committees and neighbors - Best Tune")
```

```{r var_Imp_model_tree, echo=FALSE}
plot(varImp(train_cub))
```

Resulting in:

```{r results_model_tree, echo=FALSE}
y_hat_cub<- predict(train_cub, test)
cub_results<-postResample(y_hat_cub, test$rent_amount)
cub_results
```

#### 8- Principal Component Analysis

*method = "pcr"*

```{r train_pcr, echo= FALSE, echo=FALSE, warning=FALSE, include=FALSE}
#Principal Component Analysis
#Tuning parameters:
#ncomp (#Components)
library(pls)
set.seed(35, sample.kind = "Rounding")
ini_time<- Sys.time()
train_pca<- train(rent_amount~., method= "pcr", data= train, preProcess="scale", trControl= control)
end_time<- Sys.time()
train_pca
print(end_time-ini_time)
```

```{r besttune_pca, echo=FALSE}
ggplot(train_pca, highlight= TRUE) + theme_bw() + ggtitle("N Components - Best Tune")
```


```{r pca_results, echo=FALSE}
y_hat_pca<- predict(train_pca, test)
pca_results<-postResample(y_hat_pca, test$rent_amount)
pca_results
```

#### 9- Bayesian Regularized Neural Networks

*method= "brnn"*

```{r train_brnn, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#Bayesian Regularized Neural Networks - "brnn"
#Tuning parameters:
#neurons 
library(brnn)
set.seed(35, sample.kind = "Rounding")
ini_time<- Sys.time()
train_brnn<- train(rent_amount~., method= "brnn", data= train,preProcess="scale", trControl= control)
end_time<- Sys.time()
train_brnn
print(end_time-ini_time)
```


```{r besttune_brnn, echo=FALSE}
ggplot(train_brnn, highlight= TRUE) + theme_bw() + ggtitle("Neurons - Best Tune")
```


```{r brnn_results, echo=FALSE}
y_hat_brnn<- predict(train_brnn, test)
brnn_results<-postResample(y_hat_brnn, test$rent_amount)
brnn_results
```

#### 10- Stochastic Gradient Boosting

*method= "gbm"*

```{r train_gbm, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#Stochastic Gradient Boosting
#Tuning parameters:
#n.trees (# Boosting Iterations)
#interaction.depth (Max Tree Depth)
#shrinkage (Shrinkage)
#n.minobsinnode (Min. Terminal Node Size)
library(gbm)
library(plyr)
set.seed(35, sample.kind = "Rounding")
ini_time<- Sys.time()
train_gbm<- train(rent_amount~., method= "gbm", data= train, preProcess="scale",trControl= control)
end_time<- Sys.time()
train_gbm
print(end_time-ini_time)
```


```{r besttune_gbm, echo=FALSE}
ggplot(train_gbm, highlight= TRUE) + theme_bw() + ggtitle("N.trees and Interation depth - Best Tune")
```

```{r gbm_results, echo=FALSE}
y_hat_gbm<- predict(train_gbm, test)
gbm_results<-postResample(y_hat_gbm, test$rent_amount)
gbm_results
```

Now let us compare the results of our models on the test set.

```{r model_results, echo= FALSE}
model_results <- t(data.frame(Linear_Regression= lm_results, KNearest_Neighbors= knn_results, Regression_tree= rpart_results, Random_Forest= rf_results, Random_Rborist=rb_results, SVM_Radial=svm_results, Model_tree=cub_results, Principal_Component_Analysis=pca_results, Bayesian_Reg_Neural_Networks= brnn_results, Stochastic_Gradient_Boosting=gbm_results))
model_results  
```

#### 11- Ensemble

Now we are going to combine the results of the  top 3 best RMSE results on the test set and predict with their average.  

```{r best_models, echo=FALSE, message=FALSE}
top_results <- sort(model_results[,1]) %>% head(.,3)
  top_results 
```

Let us make an ensemble model that predicts the mean value of the best 3 previous models and calculate the RMSE.

```{r ensemble_results, echo=FALSE}
ensemble<- data.frame(rf= y_hat_rf, rf= y_hat_rb, brnn= y_hat_brnn)
y_hat_ens<- rowMeans(ensemble)
ens_results<- postResample(y_hat_ens, test$rent_amount)
ens_results
```

We have a small improvement. So let us decide to use the ensemble model equal to the mean prediction of Random Forest, Random Forest Rborist and Bayesian Regularized Neural Networks as our final model.

Let us plot our final predictions and compare to actual values.

```{r ensemble_plot, echo=FALSE}
test %>% mutate(y_hat= y_hat_ens) %>%  ggplot() + geom_point(aes(y_hat,rent_amount, color=city)) + theme_bw()+ggtitle("Ensemble model Predictions") 
```

Although São Paulo is the city with the highest prevalence in the database, it is also the city that contains the largest ranges in features, requiring additional information to improve the accuracy of models.

```{r abs_erro_ ensemble, echo=FALSE}
test %>% mutate(y_hat= y_hat_ens, error= abs(rent_amount-y_hat)) %>% select(city, bathroom, hoa, property_tax, rent_amount, y_hat, error) %>% ggplot(aes(error)) + geom_histogram(binwidth = 40, fill="lightblue", color="black") + theme_bw() +ggtitle("Absolute Error Distribution")

```

```{r biggest_errors, echo=FALSE, include=FALSE}
test %>% mutate(y_hat= y_hat_ens, error= abs(rent_amount-y_hat)) %>% select(city, bathroom, hoa, property_tax, rent_amount, y_hat, error) %>% arrange(desc(error)) %>% head() %>% knitr::kable()

```

RMSE is sensitive to outliers. We have some very precise predictions. About 35% of predictions had an absolute error smaller than R$300. We can see most absolute errors were small but the effect of each error on RMSE is proportional to the size of the squared error, thus larger errors have a disproportionately large effect on RMSE. 

```{r lowest_errors, echo=FALSE, include=FALSE}
test %>% mutate(y_hat= y_hat_ens, error= abs(rent_amount-y_hat)) %>% select(city, bathroom, hoa, property_tax, rent_amount, y_hat, error) %>% arrange(error) %>% head() %>% knitr::kable()

```

```{r errors<300, include=FALSE}
test %>% mutate(y_hat= y_hat_ens, error= abs(rent_amount-y_hat)) %>% select(city, bathroom, hoa, property_tax, rent_amount, y_hat, error) %>% filter(error<=300) %>% nrow()

```

## RESULTS

As result we are going to apply our best model - ensemble - on the validation set and see how precise we are.

Validation data has 1071 observations and 13 features. As we did earlier on the build set we must remove features which we found high correlation or dependents variables. Resulting in 10 columns.

```{r dim_validation, echo=FALSE, include=FALSE}
dim(validation)
```

```{r names_validation, echo=FALSE, include=FALSE}
names(validation)
```

```{r remove_high_cor_features, echo= FALSE, include=FALSE}
validation <- select(validation,-c(area, fire_insurance, total))

```

Applying the ensemble model to validation set we obtain a RMSE equal to 2171.

```{r final_model_validation, echo= FALSE}
results_validation<- data.frame(rf= predict(train_rf, validation), rb= predict(train_rb, validation), brnn= predict(train_brnn, validation))
predict_validation <- rowMeans(results_validation)
postResample(predict_validation, validation$rent_amount)
```

Let us see the predictions by city.

```{r results_bycity, echo=FALSE, message= FALSE}
validation %>% mutate(y_hat= predict_validation) %>% group_by(city) %>% ggplot(aes(y_hat,rent_amount)) + geom_point() + geom_smooth()+facet_wrap(~city)+theme_bw()+ggtitle("Final result")
```

We were able to get very precise results on smaller rentals, and lost precision on higher ones.

The model proved to be satisfactory mainly if we took into consideration the small sample and the lack of relevant features like type of place (studio, apartment, house, country house, etc), neighborhood characteristics as comercial/residential area, average income, population, points of interest (public transportation, scholls, malls, banks, or others). 

The higher the data quality the better the accuracy of the algorithms.

## CONCLUSION

This project is one part of the final evaluation of the Data Science Professional Certificate from Harvardx-Edx, PH125.9x:Data Science: Capstone, where students should use the tools they learned during the course to solve a problem on your own and write a report explaining the whole process: the techniques used, including data cleaning, data exploration and visualization, the information obtained and modeling approaches.

For this project, we have applied machine learning techniques to solve a problem of our choice, using a dataset available at Kaggle or The UCI Machine Learning Repository. The chosen problem was the prediction of renting houses with a dataset containing information about 10.692 houses in 5 different cities in Brazil and 13 different features. Data was collected from a rental website, on 3/20/20, and it was available at Kaggle.

We tested 10 machine learning models and finally we have chosen the ensemble model, equal to the mean prediction of Random Forest, Random Forest Rborist and Bayesian Regularized Neural Networks as our final model. After applying it in an unknown dataset (validation) we have the results: RMSE =  2171, Rsquared =  0.6266 and MAE = 1286. 

Among the available features, the most important were the Property Tax, Bathroom and Hoa on most of machine learning models. We got good results but limited by the quality of the data and the absence of important information, since the economic and population characteristics of the neighborhood where the properties are located that must also be taken into account in consideration to the physical characteristics of the properties. A few outliers are responsible for much of the model error.

Some variables suggested to improve the results are: neighborhood, zipcode or adress information, type of location (studio, apartment, house, country house, bedroom), distance to nearby points of interest, such as: supermarket, pharmacy, public transportation, hospitals, schools, shopping malls, etc., existence of leisure area, gym, swimming pool, barbecue, etc. Normally this kind of information is described in an open field "Description".

It is also suggested to review the data periodically to ensure that the information provided is true. Data quality is a key factor to predict the house prices or any other machine learning problem.

